{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Grid Search\n",
    "\n",
    "- Grid search is a method of exclusively searching through a collection of possible parameters values.\n",
    "- For example if you have 2 hyperparameters you would like to tune,and 4 possible values for each hyperparameter, then a grid search over that parameter space would try all 16 possible parameter configurations\n",
    "- Number of models= number of distinct values per hyperparameter multiplied by across each hyperparameter\n",
    "- In grid search you can try parameter configuration, evaluate some metric for that configuration and pick the parameter configuration that give you the best value for the metric which is in our case will be the root mean squared error"
   ],
   "id": "46d8f1814fa3798b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T21:00:20.829683Z",
     "start_time": "2024-05-19T21:00:19.951361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ],
   "id": "6dc01604e26277e3",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T21:09:07.982437Z",
     "start_time": "2024-05-19T21:08:58.742035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "housing_data = pd.read_csv('../data/ames_housing.csv')\n",
    "X, y = housing_data[housing_data.columns[:-1]], housing_data[housing_data.columns[-1]]\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "gbm_params_grid = {'learning_rate': [0.01, 0.1, 0.5, 0.9], 'n_estimators': [200], 'subsample': [0.3, 0.5, 0.9]}\n",
    "\n",
    "gbm = xgb.XGBRegressor()\n",
    "grid_mse = GridSearchCV(estimator=gbm, param_grid=gbm_params_grid, scoring='neg_mean_squared_error', cv=4, verbose=2)\n",
    "\n",
    "grid_mse.fit(X, y)\n",
    "\n",
    "print('Best parameters found: ', grid_mse.best_params_)\n",
    "print('Lowest RMSE found: ', np.sqrt(np.abs(grid_mse.best_score_)))"
   ],
   "id": "c6d37636c7e5c2b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 12 candidates, totalling 48 fits\n",
      "[CV] END learning_rate=0.01, n_estimators=200, subsample=0.3; total time=   0.2s\n",
      "[CV] END learning_rate=0.01, n_estimators=200, subsample=0.3; total time=   0.2s\n",
      "[CV] END learning_rate=0.01, n_estimators=200, subsample=0.3; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, n_estimators=200, subsample=0.3; total time=   0.2s\n",
      "[CV] END learning_rate=0.01, n_estimators=200, subsample=0.5; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, n_estimators=200, subsample=0.5; total time=   0.2s\n",
      "[CV] END learning_rate=0.01, n_estimators=200, subsample=0.5; total time=   0.2s\n",
      "[CV] END learning_rate=0.01, n_estimators=200, subsample=0.5; total time=   0.2s\n",
      "[CV] END learning_rate=0.01, n_estimators=200, subsample=0.9; total time=   0.2s\n",
      "[CV] END learning_rate=0.01, n_estimators=200, subsample=0.9; total time=   0.2s\n",
      "[CV] END learning_rate=0.01, n_estimators=200, subsample=0.9; total time=   0.2s\n",
      "[CV] END learning_rate=0.01, n_estimators=200, subsample=0.9; total time=   0.2s\n",
      "[CV] END .learning_rate=0.1, n_estimators=200, subsample=0.3; total time=   0.1s\n",
      "[CV] END .learning_rate=0.1, n_estimators=200, subsample=0.3; total time=   0.4s\n",
      "[CV] END .learning_rate=0.1, n_estimators=200, subsample=0.3; total time=   0.1s\n",
      "[CV] END .learning_rate=0.1, n_estimators=200, subsample=0.3; total time=   0.2s\n",
      "[CV] END .learning_rate=0.1, n_estimators=200, subsample=0.5; total time=   0.1s\n",
      "[CV] END .learning_rate=0.1, n_estimators=200, subsample=0.5; total time=   0.2s\n",
      "[CV] END .learning_rate=0.1, n_estimators=200, subsample=0.5; total time=   0.2s\n",
      "[CV] END .learning_rate=0.1, n_estimators=200, subsample=0.5; total time=   0.2s\n",
      "[CV] END .learning_rate=0.1, n_estimators=200, subsample=0.9; total time=   0.2s\n",
      "[CV] END .learning_rate=0.1, n_estimators=200, subsample=0.9; total time=   0.2s\n",
      "[CV] END .learning_rate=0.1, n_estimators=200, subsample=0.9; total time=   0.1s\n",
      "[CV] END .learning_rate=0.1, n_estimators=200, subsample=0.9; total time=   0.2s\n",
      "[CV] END .learning_rate=0.5, n_estimators=200, subsample=0.3; total time=   0.1s\n",
      "[CV] END .learning_rate=0.5, n_estimators=200, subsample=0.3; total time=   0.2s\n",
      "[CV] END .learning_rate=0.5, n_estimators=200, subsample=0.3; total time=   0.1s\n",
      "[CV] END .learning_rate=0.5, n_estimators=200, subsample=0.3; total time=   0.2s\n",
      "[CV] END .learning_rate=0.5, n_estimators=200, subsample=0.5; total time=   0.1s\n",
      "[CV] END .learning_rate=0.5, n_estimators=200, subsample=0.5; total time=   0.2s\n",
      "[CV] END .learning_rate=0.5, n_estimators=200, subsample=0.5; total time=   0.2s\n",
      "[CV] END .learning_rate=0.5, n_estimators=200, subsample=0.5; total time=   0.3s\n",
      "[CV] END .learning_rate=0.5, n_estimators=200, subsample=0.9; total time=   0.2s\n",
      "[CV] END .learning_rate=0.5, n_estimators=200, subsample=0.9; total time=   0.2s\n",
      "[CV] END .learning_rate=0.5, n_estimators=200, subsample=0.9; total time=   0.3s\n",
      "[CV] END .learning_rate=0.5, n_estimators=200, subsample=0.9; total time=   0.2s\n",
      "[CV] END .learning_rate=0.9, n_estimators=200, subsample=0.3; total time=   0.2s\n",
      "[CV] END .learning_rate=0.9, n_estimators=200, subsample=0.3; total time=   0.2s\n",
      "[CV] END .learning_rate=0.9, n_estimators=200, subsample=0.3; total time=   0.2s\n",
      "[CV] END .learning_rate=0.9, n_estimators=200, subsample=0.3; total time=   0.1s\n",
      "[CV] END .learning_rate=0.9, n_estimators=200, subsample=0.5; total time=   0.2s\n",
      "[CV] END .learning_rate=0.9, n_estimators=200, subsample=0.5; total time=   0.2s\n",
      "[CV] END .learning_rate=0.9, n_estimators=200, subsample=0.5; total time=   0.2s\n",
      "[CV] END .learning_rate=0.9, n_estimators=200, subsample=0.5; total time=   0.2s\n",
      "[CV] END .learning_rate=0.9, n_estimators=200, subsample=0.9; total time=   0.2s\n",
      "[CV] END .learning_rate=0.9, n_estimators=200, subsample=0.9; total time=   0.2s\n",
      "[CV] END .learning_rate=0.9, n_estimators=200, subsample=0.9; total time=   0.2s\n",
      "[CV] END .learning_rate=0.9, n_estimators=200, subsample=0.9; total time=   0.2s\n",
      "Best parameters found:  {'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 0.5}\n",
      "Lowest RMSE found:  28518.027201847857\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Random Search\n",
    "\n",
    "- Random search is significantly different from the grid search in that the number of models that you iterate are required to iterate over does not grow as you expand the overall hyperparameter space.\n",
    "- Create possibly(infinite) range of hyperparameter value per hyperparameter that you would like to search over\n",
    "- In random search you get to decide how many models or iteration you want to try out before stopping\n",
    "- During each iteration  Random search simply involving drawing a random combination of possible values for each hyperparameter searched over and train/evaluate a model with those hyperparameters.\n",
    "- After you reached the maximum number of iteration select the hyperparameter configuration with the best evaluated score.  "
   ],
   "id": "94a469d4ac583487"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T21:21:41.560773Z",
     "start_time": "2024-05-19T21:21:23.471851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "gbm_params_grid = {'learning_rate': np.arange(0.05, 1.05, .05), 'n_estimators': [200],\n",
    "                   'subsample': np.arange(0.05, 1.05, 0.05)}\n",
    "gbm = xgb.XGBRegressor()\n",
    "randomized_rmse = RandomizedSearchCV(estimator=gbm, param_distributions=gbm_params_grid, n_iter=25,\n",
    "                                     scoring='neg_mean_squared_error', cv=4, verbose=1)\n",
    "randomized_rmse.fit(X, y)\n",
    "print('Best parameters found: ', randomized_rmse.best_params_)\n",
    "print('Lowest RMSE found: ', np.sqrt(np.abs(randomized_rmse.best_score_)))"
   ],
   "id": "e7eb143b80f343dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 25 candidates, totalling 100 fits\n",
      "Best parameters found:  {'subsample': 0.2, 'n_estimators': 200, 'learning_rate': 0.05}\n",
      "Lowest RMSE found:  28928.97143360788\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Limits of GridSearchCV and RandomSearchCV\n",
    "- Grid Search\n",
    " - Number of models must build with every additional new parameter grows quickly\n",
    "\n",
    "- Random Search\n",
    "  - Parameter space to explore can be massive\n",
    "  - Randomly jumping throughout the space looking for a best result becomes a waiting time.\n",
    "     "
   ],
   "id": "6e0999798b6df20c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
